{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    "<pre> NYU Paris            <i> Artificial intelligence - Fall 2023 </i></pre>\n",
    "* * *\n",
    "\n",
    "\n",
    "<h1 align=\"center\"> Lab 7: Introduction to deep learning </h1>\n",
    "\n",
    "<pre align=\"left\"> October 12th 2023               <i> Author: Hicham Janati </i></pre>\n",
    "* * *\n",
    "\n",
    "\n",
    "##### Goals:\n",
    "- Discover pytorch\n",
    "- Create your own neural network\n",
    "- Adapt the complexity of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "people = fetch_lfw_people(min_faces_per_person=60, resize=0.7)\n",
    "image_shape = people.images[0].shape\n",
    "X = people.images.reshape(len(people.images), -1)\n",
    "y = people.target\n",
    "X = X / 255\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# split the data in training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=0)\n",
    "\n",
    "pca = PCA(100)\n",
    "pca.fit(X_train)\n",
    "\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "X_test = torch.tensor(X_test)\n",
    "\n",
    "y_train = torch.tensor(y_train)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "print(X_train.shape, X_test.shape, len(np.unique(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With pytorch, a neural netwrok is an boject that inherist from `nn.Module`. It must contain:\n",
    "\n",
    "- a constructor ```___init___``` which specifies its attributes (number of layers, neurones etc)\n",
    "- a forward function ```forward``` that computes the output of the neural net given a data input `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \"\"\"Reseau de neurones simple.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear_layer = nn.Linear(100, 8) \n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.linear_layer(X)\n",
    "        \n",
    "        torch.nn.LogSoftmax(dim=-1)(X).squeeze()\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "What does this first neural net above correspond to ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating our neural network, we need to write a function that performs the 'fit' operation i.e solve the optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_neural_net(neural_net,\n",
    "                   X_train, X_test, y_train, y_test,\n",
    "                   learning_rate=0.005,\n",
    "                   weight_decay=0.,\n",
    "                   max_iter=1000,\n",
    "                   device='cpu',\n",
    "                   verbose=True):\n",
    "    \"\"\"\n",
    "    Function that performs back-propagation (gradient descent)\n",
    "    to optimize the neural net.\n",
    "    neural_net: Instance of NeuralNet\n",
    "    learning_rate: gradient step size\n",
    "    weight_decay: regularization parameter of L2 regularization\n",
    "    max_iter: nombre maximum d'it√©rations\n",
    "    device: 'cpu' or 'cuda:0'\n",
    "\n",
    "    Returns\n",
    "    scores: dictionary of s\n",
    "    \"\"\"\n",
    "\n",
    "    loss_func = nn.CrossEn().to(device)\n",
    "    neural_net = neural_net.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(neural_net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    loss_train, loss_test = [], []\n",
    "    accuracy_train, accuracy_test = [], []\n",
    "\n",
    "    if verbose:\n",
    "        strings = [\"Iteration\", \"Accuracy Train\", \"Accuracy Test\"]\n",
    "        strings = [s.center(13) for s in strings]\n",
    "        strings = \" | \".join(strings)\n",
    "        print(strings)\n",
    "\n",
    "    for ii in range(max_iter):\n",
    "        # Train\n",
    "        neural_net.train()\n",
    "        optimizer.zero_grad()\n",
    "        log_proba_pred_train = neural_net(X_train)\n",
    "        loss_train_ = loss_func(log_proba_pred_train, y_train)  #### compute the loss\n",
    "        loss_train_.backward()  ### compute the gradients and do back propagation\n",
    "        optimizer.step()    # gradient step\n",
    "\n",
    "\n",
    "      # Test\n",
    "      # turn neural net to evaluation mode to not keep computing gradients\n",
    "        neural_net.eval()\n",
    "        with torch.no_grad():\n",
    "            log_proba_pred_test = neural_net(X_test)\n",
    "            y_pred_train = torch.exp(log_proba_pred_train).argmax(1) ### On passes des log probas de chaque classe a une prediction\n",
    "            y_pred_test = torch.exp(log_proba_pred_test).argmax(1)\n",
    "            loss_test_ = loss_func(log_proba_pred_test, y_test).item()  #### On calcule la perte\n",
    "            accuracy_test_ = (y_pred_test == y_test).float().mean().item() #### On calcule la precision test\n",
    "            accuracy_train_ = (y_pred_train == y_train).float().mean().item() #### On calcule la precision train\n",
    "\n",
    "        if verbose and ii % 50 == 0:\n",
    "            strings = [ii, accuracy_train_, accuracy_test_] # On affiche des trucs \n",
    "            strings = [str(s).center(14) for s in strings]\n",
    "            strings = \" | \".join(strings)\n",
    "            print(strings)\n",
    "\n",
    "        loss_train.append(loss_train_.item())\n",
    "        loss_test.append(loss_test_)\n",
    "        accuracy_train.append(accuracy_train_)\n",
    "        accuracy_test.append(accuracy_test_)\n",
    "\n",
    "    metrics = [[loss_train, loss_test],\n",
    "              [accuracy_train, accuracy_test]]\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def make_report(neural_net, X_test):\n",
    "    neural_net.eval()\n",
    "    y_pred = neural_net(X_test).detach().cpu().numpy().argmax(1)\n",
    "    print(classification_report(y_test.cpu().numpy(), y_pred))\n",
    "\n",
    "neural_net = NeuralNet()\n",
    "metrics = fit_neural_net(neural_net, X_train, X_test, y_train, y_test)\n",
    "make_report(neural_net, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Display the loss and accuracy of both train and test as a function of the training epochs (iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Modify the neural net above and create a more sophisticated one with 1 hidden layer with a ReLU activation function containing 20 neurones. Does the performance improve ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "class NeuralNet_v2(nn.Module):\n",
    "    \"\"\"Neural net with 1 hidden layer .\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # to do \n",
    "        \n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.linear_layer(X)\n",
    "        # todo \n",
    "        X = torch.nn.LogSoftmax(dim=-1)(X).squeeze()\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "The following net is a specific neural net known as convolutional neural networks. Train the model and see if the performance is better. Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \"\"\"Basic ConvNet\"\"\"\n",
    "\n",
    "    def __init__(self, n_outputs=10, debug=False):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(2304, 100)\n",
    "        self.fc2 = nn.Linear(100, n_outputs)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.debug = debug\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        # x = self.conv2_drop(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # flatten over channel, height and width = 1600\n",
    "        x = x.view(-1, x.size(1) * x.size(2) * x.size(3))\n",
    "\n",
    "        if self.debug:  # trick to get the size of the first FC\n",
    "            print(\"### DEBUG: Shape of last convnet=\", x.shape,\n",
    "                  \". FC size=\", np.prod(x.shape[1:]))\n",
    "\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = torch.nn.LogSoftmax(dim=-1)(x).squeeze()\n",
    "        return x\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "conv_net = ConvNet()\n",
    "optimizer = optim.Adam(conv_net.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "X_train_s = torch.tensor(scaler.fit_transform(X_train)).reshape(-1, 1, 32, 32)\n",
    "X_test_s = torch.tensor(scaler.transform(X_test)).reshape(-1, 1, 32, 32)\n",
    "y_train = torch.tensor(y_train)\n",
    "n_iters = 0\n",
    "\n",
    "losses = []\n",
    "for ii in range(n_iters):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred_train = conv_net(X_train_s)\n",
    "    risque = loss_func(y_pred_train, y_train)  #### On calcule la perte\n",
    "    losses.append(risque.detach())\n",
    "    risque.backward()  ### On calcule les gradients avec la back-propagation\n",
    "    optimizer.step()    # pas descente de gradient\n",
    "    \n",
    "\n",
    "conv_net.eval()\n",
    "y_pred = conv_net(X_test_s).detach().cpu().numpy().argmax(1)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
