{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8iwPmgjhJEd"
   },
   "source": [
    "# Training a character language model and studying various ways of generating text\n",
    "\n",
    "**Author: matthieu.labeau@telecom-paris.fr**\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "- We will train a network to predict a next character given an input sequence of characters, and use it to generate new sequences.\n",
    "- We will strictly work with local (and not structured) prediction - however, we will look into relatively simple heuristics to improve the \"structure\" of our generation: *temperature* sampling, *beam search*, *top-k* sampling, *top-p* sampling, drop-out input data.\n",
    "- We will use ```keras```to build the model based on a LSTM, which will use simple features (one-hot vector representing previous characters) to predict the next characters. We will use a small model to avoid training for too long.\n",
    "- We will use a small dataset (poetry, from project Gutenberg) - you can use any data you prefer, as long as you are able to train the model on it.\n",
    "- Even with a small dataset and a small model, training may be long (~1 min/epoch on GPU). If you can use a computing infrastructure, like Google colab, it may be more practical - and you probably can obtain better results by using a bigger model and a larger dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6bIITyO78agJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWvSyqKyhJEk"
   },
   "source": [
    "### Obtaining the data\n",
    "- We download directly the ebook from project Gutenberg - you can get any other text you would prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/lilyque/anaconda3/lib/python3.11/site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.15.0 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.25.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/lilyque/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "curl: (7) Failed to connect to www.gutenberg.org port 443 after 8 ms: Couldn't connect to server\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://www.gutenberg.org/cache/epub/100/pg100.txt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZFtJnsqhJEo",
    "outputId": "638d7fc9-2c69-465d-9dc7-e1b79403521d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.gutenberg.org/cache/epub/100/pg100.txt\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "URL fetch failure on https://www.gutenberg.org/cache/epub/100/pg100.txt: None -- [Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[1;32m   1349\u001b[0m               encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1286\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1332\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1331\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders(body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1281\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1041\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m \n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:979\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 979\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1451\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnect to a host on a given (SSL) port.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1451\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:945\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    944\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n\u001b[0;32m--> 945\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection(\n\u001b[1;32m    946\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address)\n\u001b[1;32m    947\u001b[0m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:851\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_errors:\n\u001b[0;32m--> 851\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ExceptionGroup(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_connection failed\u001b[39m\u001b[38;5;124m\"\u001b[39m, exceptions)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:836\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[1;32m    835\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m--> 836\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/data_utils.py:347\u001b[0m, in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     urlretrieve(origin, fpath, DLProgbar())\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/data_utils.py:85\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m response \u001b[38;5;241m=\u001b[39m urlopen(url, data)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fd:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(req, data)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_open, protocol, protocol \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    537\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_open\u001b[39m\u001b[38;5;124m'\u001b[39m, req)\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    495\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_open(http\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mHTTPSConnection, req,\n\u001b[1;32m   1392\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context, check_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_hostname)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:1351\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1352\u001b[0m r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 61] Connection refused>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_file\n\u001b[1;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.gutenberg.org/cache/epub/100/pg100.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m path \u001b[38;5;241m=\u001b[39m get_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpg100.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, origin\u001b[38;5;241m=\u001b[39murl)\n\u001b[1;32m      5\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m , encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/data_utils.py:351\u001b[0m, in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg\u001b[38;5;241m.\u001b[39mformat(origin, e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmsg))\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg\u001b[38;5;241m.\u001b[39mformat(origin, e\u001b[38;5;241m.\u001b[39merrno, e\u001b[38;5;241m.\u001b[39mreason))\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(fpath):\n",
      "\u001b[0;31mException\u001b[0m: URL fetch failure on https://www.gutenberg.org/cache/epub/100/pg100.txt: None -- [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import get_file\n",
    "url = 'https://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "path = get_file('pg100.txt', origin=url)\n",
    "\n",
    "f = open(path, 'r' , encoding = 'utf8')\n",
    "lines = f.readlines()\n",
    "text = []\n",
    "\n",
    "\n",
    "start = False\n",
    "for line in lines:\n",
    "    line = line.strip().lower() # Remove blanks and capitals\n",
    "    if(\"*** START OF THE PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM SHAKESPEARE ***\".lower() in line and start==False):\n",
    "        start = True\n",
    "    if(\"*** END OF THE PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM SHAKESPEARE ***\".lower() in line):\n",
    "        break\n",
    "    if(start==False or len(line) == 0):\n",
    "        continue\n",
    "    text.append(line)\n",
    "\n",
    "f.close()\n",
    "text = \" \".join(text)\n",
    "voc_chars = sorted(set([c for c in text]))\n",
    "nb_chars = len(voc_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gq9rNoU5hJEs",
    "outputId": "98f4eaa4-eeb0-441b-89ae-21b71f39729f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(lines[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lines' is not defined"
     ]
    }
   ],
   "source": [
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZbgX7SZhJEu"
   },
   "source": [
    "### Keeping track of possible characters\n",
    "- Using a ```set```, create a sorted list of possible characters\n",
    "- Create two dictionnaries, having characters and corresponding indexes as {key: value}, and reverse.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "chars = [a, b, c]\n",
    "```\n",
    "\n",
    "```python\n",
    "chars_indices = {a: 0, b: 1, c: 2}\n",
    "```\n",
    "\n",
    "```python\n",
    "indices_chars = {0: a, 1: b, 2: c}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "udNyLiiEhJEw"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorpus length:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(text))\n\u001b[1;32m      3\u001b[0m chars \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal number of characters:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(chars))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "print('Corpus length:', len(text))\n",
    "\n",
    "chars = ...\n",
    "print('Total number of characters:', len(chars))\n",
    "char_indices = ...\n",
    "indices_char = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWytAwT2iwjf"
   },
   "source": [
    "#### Parenthesis: looking at words\n",
    "\n",
    "- Try to look at tokenization schemes: what are the most frequent words if we use *whitespace* tokenization ?\n",
    "- What are the most frequent words in this dataset using smarter tokenization ? You can a tokenizer from NLTK: ```nltk.word tokenize```.\n",
    "- Make a plot of rank vs. word count. Does Zipf's Law seem to hold ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yxbc7YZQ2rsu"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Apply whitespace tokenization\n",
    "tokens = ...\n",
    "\n",
    "# Get frequencies and sort words according to them\n",
    "freq = ...\n",
    "ordered_word_list = ...\n",
    "print(ordered_word_list[:100])\n",
    "\n",
    "del tokens\n",
    "del freq\n",
    "del ordered_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CO1FmkY95Xyd"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53QEqxMF3KXd"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens = ...\n",
    "\n",
    "freq = ...\n",
    "ordered_word_list = ...\n",
    "print(ordered_word_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WK8TRLd6kyRv"
   },
   "outputs": [],
   "source": [
    "# Create an array containing the word rank in the first dimension, and its count in the second.\n",
    "rank_counts = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7YxWvrN12fIz"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.title('Word counts versus rank')\n",
    "plt.scatter(rank_counts[:,0], rank_counts[:,1])\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print('Vocabulary size: %i' % len(freq))\n",
    "print('Part of the corpus by taking the \"x\" most frequent words:')\n",
    "for i in range(1000, len(freq), 1000):\n",
    "    print('%i : %.2f' % (i, np.sum(rank_counts[:i, 1]) / np.sum(rank_counts[:,1]) ))\n",
    "\n",
    "del tokens\n",
    "del freq\n",
    "del ordered_word_list\n",
    "del rank_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBvvgFLUhJEy"
   },
   "source": [
    "### Creating a model: a first, simple version\n",
    "\n",
    "This model will take as input **a fixed number of characters** and output the next one. We will treat the model as a *black box* and leave its innner working for later.\n",
    "\n",
    "#### Creating training data\n",
    "- We will represent characters using *one-hot vectors*. Hence, the i-th character of n possible characters will be represented by a vector of length $n$, containing $0$ expect for a $1$ in position $i$. Following our previous examples, ```a = [1, 0, 0]``` and ```b = [0, 1, 0]```.\n",
    "- Hence, a sequence of characters is a list of one-hot vectors. Our goal will be to predict, given an input sequence of fixed length (here, this length is given by ```maximum_seq_length```) the next character. Hence, we need to build two lists: ```sentences```, containing the input sequences, and ```next_char``` the characters to be predicted.\n",
    "- We do not necessarily need to take all possible sequences. We can select one every ```time_step``` steps.\n",
    "\n",
    "Example: Using the previous dictionnaries, the sequence:\n",
    "```'acabbaccaabba'``` with ```maximum_seq_length = 4``` and ```time_step = 2``` would give the following lists:\n",
    "\n",
    "```python\n",
    "sentences = ['acab', 'abba', 'bacc', 'ccaa', 'aabb']\n",
    "```\n",
    "\n",
    "```python\n",
    "next_char = ['b', 'c', 'a', 'b', 'a']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5mJDBLchJEz"
   },
   "outputs": [],
   "source": [
    "maximum_seq_length = 30\n",
    "time_step = 4\n",
    "sentences = []\n",
    "next_char = []\n",
    "for i in range(...):\n",
    "    sentences.append(...)\n",
    "    next_char.append(...)\n",
    "print('Number of Sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9dG3M9ahJE2"
   },
   "source": [
    "#### Creating training tensors\n",
    "- We need to transform these lists into tensors, using one-hot vectors to represent characters.\n",
    "- We will need 3 dimensions for the training examples from ```sentences```: the number of examples, the length of the sequence, and the dimension of the one-hot vector\n",
    "- This is reduced to 2 dimensions for the ```next_char```: number of examples and one-hot vector.\n",
    "\n",
    "Example: the previous ```sentences``` would become:\n",
    "\n",
    "```python\n",
    "X = [[[1, 0, 0],\n",
    "      [0, 0, 1],\n",
    "      [1, 0, 0],\n",
    "      [0, 1, 0]],\n",
    "     [[1, 0, 0],\n",
    "      [0, 1, 0],\n",
    "      [0, 1, 0],\n",
    "      [1, 0, 0]],\n",
    "     [[0, 1, 0],\n",
    "      [1, 0, 0],\n",
    "      [0, 0, 1],\n",
    "      [0, 0, 1]],\n",
    "     [[0, 0, 1],\n",
    "      [0, 0, 1],\n",
    "      [1, 0, 0],\n",
    "      [1, 0, 0]],\n",
    "     [[1, 0, 0],\n",
    "      [1, 0, 0],\n",
    "      [0, 1, 0],\n",
    "      [0, 1, 0]]]\n",
    "```\n",
    "       \n",
    "```python\n",
    "y = [[0, 1, 0],\n",
    "     [0, 0, 1],\n",
    "     [1, 0, 0],\n",
    "     [0, 1, 0],\n",
    "     [1, 0, 0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCXjwGtahJE3"
   },
   "outputs": [],
   "source": [
    "X = np.zeros((len(sentences), maximum_seq_length, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
    "# Loop over the sentences\n",
    "for ...\n",
    "    # Loop over the characters\n",
    "    for ...\n",
    "        # Put the right value of X to 1\n",
    "        ...\n",
    "    # Put the right value of y to 1\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktQugjFcFUHs"
   },
   "source": [
    "#### Implement the model\n",
    "\n",
    "In order to implement the model as simply as possible, we will use ```keras```. It allows to create models with only a few lines of code.\n",
    "First, we will create a very simple model based on a **LSTM**, which is a *recurrent* architecture. Note that one the strength of a recurrent architecture is to allow for inputs of varying length - here, to simplify data processing, we will keep a **fixed input size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzIGLB9_hJE5"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping\n",
    "from keras.metrics import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqKe6y1ShJE5"
   },
   "source": [
    "We need to create a LSTM model that takes directly out inputs from ```X``` and try to predict one-hot vectors from ```y```.\n",
    "- What are the input and output dimensions ?\n",
    "  - ```X```: size of the dataset $\\times$ maximum sequence length $\\times$ vocabulary size\n",
    "  - ```y```: size of the dataset $\\times$ vocabulary size\n",
    "- The model should be made with a ```LSTM``` layer, and a ```Dense``` layer followed by a softmax activation function. Work out the intermediate dimensions:\n",
    "  - ```X``` $\\rightarrow$ (LSTM) $\\rightarrow$ ```h``` $\\rightarrow$ (Dense) $\\rightarrow$ ```s``` $\\rightarrow$ (softmax) $\\rightarrow$ ```pred```\n",
    "  - Look at layers arguments and find out to proper ```input_shape``` for the ```LSTM``` layer and the proper size for the ```Dense``` layer.\n",
    "  - You can use 128 as the size of hidden states for the ```LSTM```.\n",
    "- We will minimize ```cross-entropy(pred, y)```. Use the ```categorical_crossentropy``` loss, with the optimizer of your preference (for example, ```adam```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmrllaH_hJE6"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maximum_seq_length, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3nlMjqGHCYc"
   },
   "source": [
    "You will now only need a few functions to use this model:\n",
    "- ```model.fit```, which you will call on the appropriately processed data ```X, y```\n",
    "- ```model.predict```, which you will use on an input **of the same dimension of X** to output the probabilities. That includes the *first one*, corresponding to the number of examples in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5jZ9jv3hJE7"
   },
   "source": [
    "### Create functions to generate text with our model\n",
    "- We use the output of our model (vector of probabilities on characters) to select the next most probable character (with the ```argmax```)\n",
    "- We need to transform an input text into an input tensor, as before (taking the right length, the last ```maximum_seq_length``` characters)\n",
    "- We need to transform back the most probable index into a character and add it to our text.\n",
    "- This must be looped ```num_generated``` times, each time obtaining a new input tensor from the new input sequence (which has the character we previously predicted at the end !)\n",
    "\n",
    "We can begin to write a function facilitating the transfer between text and tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CeiDQDnqJSwu"
   },
   "outputs": [],
   "source": [
    "def get_tensor(sentence, maximum_seq_length, voc):\n",
    "    x = np.zeros(...)\n",
    "    # Fill out x appropriately\n",
    "    ...\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOepuasUJS58"
   },
   "source": [
    "You have now what is necessary to fill out ```generate_next```.\n",
    "\n",
    "The following function (```end_epoch_generate```) is here to facilitate automatic generation at the end of each epoch, so you can monitor of generation changes as the model trains. It calls the ```generate_next``` function upon each sequence of text in ```texts_ex```. The only element in this list right now comes from the training data - you can add your own. We also use ```EarlyStopping``` to stop training when validation loss does not decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMBXDllYhJE7"
   },
   "outputs": [],
   "source": [
    "def generate_next(model, text, num_generated=120):\n",
    "    generated = text\n",
    "    sentence = text[-maximum_seq_length:]\n",
    "    for i in range(num_generated):\n",
    "        # Obtain the representation for the sentence, the prediction, the index of the better character and the character itself.\n",
    "        x = ...\n",
    "        predictions = ...\n",
    "        next_index = ...\n",
    "        next_char = ...\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    return(generated)\n",
    "\n",
    "def end_epoch_generate(epoch, _):\n",
    "    print(' Generating text after epoch: %d' % (epoch+1))\n",
    "    texts_ex = [\"From fairest creatures we desire increase,\"]\n",
    "    for text in texts_ex:\n",
    "        sample = generate_next(model, text.lower())\n",
    "        print('%s' % (sample))\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=2,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIp50FeYhJE8"
   },
   "source": [
    "Test generation with the model not yet trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "583GgxOGhJE8"
   },
   "outputs": [],
   "source": [
    "text_ex = \"From fairest creatures we desire increase,\"\n",
    "generate_next(model, text_ex.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcQJvVZyhJE9"
   },
   "outputs": [],
   "source": [
    "model.fit(X, y,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          validation_split = 0.2,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=end_epoch_generate), early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUeWKl-_hJE9"
   },
   "source": [
    "#### Sampling with our model\n",
    "- Now, instead of simply selecting the most probable next character, we would like to be able to draw a sample from the distribution output by the model.\n",
    "- To better control the generation, we would like to use the argument ```temperature```, to smooth the distribution.\n",
    "- Use the ```multinomial``` function from the ```random``` package to draw samples.\n",
    "- Integrate this into a function ```generate_sample``` that be almost exactly like ```generate_next```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCQBbrIghJE-"
   },
   "outputs": [],
   "source": [
    "def sample(predictions, temperature):\n",
    "    # From the prediction, apply the temperature to reweights the probabilities and sample.\n",
    "    predictions = np.asarray(predictions).astype('float64')\n",
    "    log_predictions = np.log(predictions) / temperature\n",
    "    predictions = np.exp(log_predictions)\n",
    "    predictions = predictions / np.sum(predictions)\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def generate_sample(model, text, num_generated=120, temperature=1.0):\n",
    "    generated = text\n",
    "    sentence = text[-maximum_seq_length:]\n",
    "    for i in range(num_generated):\n",
    "        x = ...\n",
    "        predictions = ...\n",
    "        next_index = sample_function(predictions, temperature)\n",
    "        next_char = ...\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    return(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtwRJag3hJE-"
   },
   "outputs": [],
   "source": [
    "generate_sample(model, text_ex.lower(), temperature = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOeuhvq_hJE_"
   },
   "source": [
    "#### Generate text with the beam algorithm\n",
    "- Complete this function implementing the beam procedure.\n",
    "- We need to loop for each character we want to generate, keeping track of the best ```beam_size``` sequences at the most.\n",
    "- Besides keeping track of past generated character for each of these ```beam_size``` sequences, we need to keep track of their log-probability.\n",
    "- This is done by, at each loop, keeping the ```beam_size```best predictions for each of the ```beam_size``` sequences, computing the log-probabilities of the newly formed (```beam_size```)$^2$ , and keeping the overall ```beam_size``` best new sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRM0L4S6hJE_"
   },
   "outputs": [],
   "source": [
    "def generate_beam(model, text, beam_size=5, num_generated=120):\n",
    "    generated = text\n",
    "    sentence = text[-maximum_seq_length:]\n",
    "    # Initialization of the beam with log-probabilities for the sequence\n",
    "    current_beam = [(0, [], sentence)]\n",
    "\n",
    "    for l in range(num_generated):\n",
    "        all_beams = []\n",
    "        for prob, current_preds, current_input in current_beam:\n",
    "            x = ...\n",
    "            prediction = ...\n",
    "            # beam_size best predictions !\n",
    "            possible_next_chars = ...\n",
    "            # Add to the beams: (the probability of the sequence, the sequence of indexes generated, the full sequence of characters)\n",
    "            all_beams += [\n",
    "                (...,\n",
    "                 ...,\n",
    "                 ...\n",
    "                )\n",
    "                for ... in possible_next_chars]\n",
    "\n",
    "        # Sort the beams according to their probability and keep the beam_size best\n",
    "        current_beam = ...\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVNP5NHFhJFA"
   },
   "outputs": [],
   "source": [
    "generate_beam(model, text_ex.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_wrNn_erDZ9"
   },
   "source": [
    "#### Reranking\n",
    "If we would like to use our outputs for a task, a common strategy is to generate a large number of them, and select the one that maximizes the relevant metric: this is **re-ranking**.\n",
    "\n",
    "First, we need an evaluation measure. For now, we can use **perplexity**: given how cross-entropy is computed in ```keras```, perplexity is simply *the exponential of the mean cross-entropy over the sequence*.\n",
    "\n",
    "To compute the cross-entropy, you need both the true ```y``` and the corresponding ```pred``` output by the model. While the most efficient would be to gather these while sampling, we can re-compute it for any sentence with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wT2OGm9dwFCX"
   },
   "outputs": [],
   "source": [
    "def get_preds(sentence):\n",
    "    # Reconstitute the inputs and outputs from the sentence: first as list of characters\n",
    "    eval_input = []\n",
    "    eval_target = []\n",
    "    for i in range(0, len(sentence) - maximum_seq_length):\n",
    "        eval_input.append(sentence[i: i + maximum_seq_length])\n",
    "        eval_target.append(sentence[i + maximum_seq_length])\n",
    "\n",
    "    # Then as tensors\n",
    "    eval_X = np.zeros(...)\n",
    "    for ...\n",
    "        for ...\n",
    "            ...\n",
    "    ...\n",
    "\n",
    "    eval_pred = ...\n",
    "    return eval_y, eval_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4f_CnqvG2ypO"
   },
   "outputs": [],
   "source": [
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = ...\n",
    "    perplexity = ...\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMRrMzcuwFpW"
   },
   "outputs": [],
   "source": [
    "def re_rank(sentence):\n",
    "    eval_y, eval_pred = get_preds(sentence)\n",
    "    return perplexity(eval_y, eval_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGv4qxltbOVK"
   },
   "source": [
    "You can then generate a bunch of sentences, obtain their perplexity and sort them accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VWHb5nG9_t8"
   },
   "outputs": [],
   "source": [
    "reranked = dict()\n",
    "for k in np.arange(5,10):\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        sentence = generate_sample(model, text_ex.lower(), sample_top_k, k, temperature = 0.7)\n",
    "        p = re_rank(sentence)\n",
    "        reranked[sentence] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQd_5M6o4tcp"
   },
   "outputs": [],
   "source": [
    "print(sorted(reranked, key=reranked.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFR9563Z5qz_"
   },
   "outputs": [],
   "source": [
    "print(reranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LADfuc6w8ohV"
   },
   "outputs": [],
   "source": [
    "del X\n",
    "del y\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEPEjdxbm2N6"
   },
   "source": [
    "### Create a second model: with embeddings\n",
    "\n",
    "This model will have a key difference with the previous one: we will use **dense embeddings** instead of one-hot representations.\n",
    "This means that:\n",
    "- In ```X```, we will use the index of the character instead of an one-hot vector. For example, the following ```['acab', 'abba', 'bacc', 'ccaa', 'aabb']``` will be represented as ```[[0, 2, 0, 1], [0, 1, 1, 0], [1, 0, 2, 2], [2, 2, 0, 0]]```.\n",
    "- We will need another input layer before the LSTM. Instead of ```Dense```, keras has a dedicated layer: ```Embedding```.\n",
    "\n",
    "Create the dataset, adapt the generation function, create the new model, and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-WmQ-uetVJk9"
   },
   "outputs": [],
   "source": [
    "X_emb = np.zeros(...)\n",
    "y_emb = np.zeros(...)\n",
    "for ...\n",
    "    for ...\n",
    "        ...\n",
    "    ...\n",
    "\n",
    "print('X_emb shape:', X_emb.shape)\n",
    "print('y_emb shape:', y_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EY0SoWgzcsd"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7qNLscWhJFE"
   },
   "outputs": [],
   "source": [
    "def generate_next_emb(model, text, num_generated=120):\n",
    "    generated = text\n",
    "    sentence = text[-maximum_seq_length:]\n",
    "    char_idxs = [[char_indices[char] for char in sentence]]\n",
    "    for i in range(num_generated):\n",
    "        x = ...\n",
    "        predictions = ...\n",
    "        next_index = ...\n",
    "        next_char = ...\n",
    "        generated += next_char\n",
    "        char_idxs = [char_idxs[0][1:] + [next_index]]\n",
    "    return(generated)\n",
    "\n",
    "def end_epoch_generate(epoch, _):\n",
    "    print('\\n Generating text after epoch: %d' % (epoch+1))\n",
    "    texts_ex = [\"From fairest creatures we desire increase,\"]\n",
    "    for text in texts_ex:\n",
    "        sample = generate_next_emb(model_emb_m2m, text)\n",
    "        print('%s' % (sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEGCsBpvhJFE"
   },
   "outputs": [],
   "source": [
    "model_emb_m2m = Sequential()\n",
    "model_emb_m2m.add(...)\n",
    "model_emb_m2m.add(LSTM(128, input_shape=(maximum_seq_length, 32), return_sequences=False))\n",
    "model_emb_m2m.add(Dense(len(chars)))\n",
    "model_emb_m2m.add(Activation('softmax'))\n",
    "\n",
    "model_emb_m2m.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzM17QC3hJFF"
   },
   "outputs": [],
   "source": [
    "model_emb_m2m.fit(X_emb, y_emb,\n",
    "                  batch_size=128,\n",
    "                  epochs=5,\n",
    "                  validation_split = 0.2,\n",
    "                  callbacks=[LambdaCallback(on_epoch_end=end_epoch_generate)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PidmpsnbhJFF"
   },
   "outputs": [],
   "source": [
    "generate_next_emb(model_emb_m2m, \"From fairest creatures we desire increase,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KyLbTcem5qE"
   },
   "outputs": [],
   "source": [
    "def get_tensor_emb(sentence, maximum_seq_length, voc):\n",
    "    x = ...\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RthihH0u-CpJ"
   },
   "outputs": [],
   "source": [
    "def generate_sample_emb(model, text, num_generated=120, temperature=1.0):\n",
    "    generated = text\n",
    "    sentence = text[-maximum_seq_length:]\n",
    "    for i in range(num_generated):\n",
    "        x = ...\n",
    "        predictions = ...\n",
    "        next_index = sample_function(predictions, temperature)\n",
    "        next_char =  ...\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    return(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKJHS47L_FFQ"
   },
   "outputs": [],
   "source": [
    "generate_sample_emb(model_emb_m2m, text_ex.lower(), temperature = 0.7)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
